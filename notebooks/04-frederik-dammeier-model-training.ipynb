{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Model training is handled by scripts provided by the Tensorflow object detection API, thus we do not have any training code in a notebook. The process, however, will be explained here.\n",
    "\n",
    "### General Links\n",
    "- [TensorFlow object detection API (git)](https://github.com/tensorflow/models/tree/master/research/object_detection)\n",
    "- [TensorFlow object detection API (unoffical tutorial)](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)\n",
    "- [TensorFlow object detection model zoo (COCO pretrained)](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some variables and a utility function are defined which make it easier to execute the training script with the correct parameters. (This neat little trick is inspired by Christophs and Linus' work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-28 12:10:27.299606: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_MODEL_MAIN = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/scripts/model_main_tf2.py'\n",
    "MODEL_DIR = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/'\n",
    "CONFIG_PATH = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/pipeline.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_query():\n",
    "    return f\"python {PATH_TO_MODEL_MAIN} --model_dir={MODEL_DIR} --pipeline_config_path={CONFIG_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python /home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/scripts/model_main_tf2.py --model_dir=/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/ --pipeline_config_path=/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/pipeline.config'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_training_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a more detailed look at the training process will be provided. \n",
    "## 2D Bounding Box Object Detection\n",
    "The task on hand is to detect objects in color images by drawing a rectangular bounding box around them. It is usually devided into two subtasks, that are individually adressed during model desing:\n",
    "1. Bounding box regression, the task of identifying regions whithin the image that are likely to contain objects (localization)\n",
    "2. Classification of those regions of iterest (classification)\n",
    "As our problem is single class, we only have to detect whether a given region contains an object or not. \n",
    "### Models\n",
    "A multitude of models exist for bounding box object detection, some famous ones being\n",
    "- Faster R-CNN\n",
    "- SSD (Single-Stage-Detector)\n",
    "- EfficientDet\n",
    "- YOLO (v1, v2, v3, v4, v5, X)\n",
    "\n",
    "The most performant \"state of the art\" are the newer YOLO models and EfficientDet. All of the above are based on CNNs, recently however, transformer-based approaches have been published to literature as well.\n",
    "\n",
    "The largest difference in model architecture is usually whether they use a single-stage or multi-stage design. Single stage detectors combine the localization and classification parst into a single network component, while two stage detectors use seperate localization and classifcation components.\n",
    "\n",
    "Object detection models usually use the same feature extraction networks as their backbones as image-classifies do. These backbones are usually pretrained on ImageNet, a large-scale classification dataset. Object detectors themselve are usually pretrained and evaluated on the COCO-Dataset using the COCO mAP metric. \n",
    "\n",
    "The TensorFlow object detection API offers a collection of object detection models pretrained on COCO which we will use in this project.\n",
    "### Evaluation of 2D Object Detectors\n",
    "After an object detector has been trained, its predictions are evaluated on a test or validation set. The predictions consist of the class, localization and confidence. These values are compared to the provided ground truth. In the case of object detection, there can be multiple instances of multiple classes within a single image. Thus, a simple measurement for classification accuracy is not appropriate (Everingham et al, 2010). Instead, both the localization and classification of objects have to be evaluated. The localization is only considered if the prediction confidence for a proposed bounding box exceeds a defined threshold. If that is the case, the localization error is determined by the calculation of the intersection of union (IoU). The IoU is defined as the overlapping of the prediction $B_p$ and ground truth $B_{gt}$ bounding boxes. With $area(B_p \\cap B_{gt})$ being the area of the intersection and $area(B_p \\cup B_{gt})$ the area of the union, the IoU is calculated as\n",
    "$$IoU = \\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$$\n",
    "(Everingham et al, 2010).\n",
    "With the measurement of class confidence and IoU, a proposed object can be classified into three states. The detection is a True Positive (TP) if the confidence and IoU exceed a defined threshold and thus the prediction is defined as correct. If the IoU is too low or zero or the predicted class is wrong, this is called a False Positive (FP). False negative (FN) detections are objects that are not detected at all or do not exceed the defined confidence threshold. Using these metrics, the precision and recall can be defined as follows:\n",
    "$$Precision = \\frac{TP}{TP+FP},$$\n",
    "$$Recall = \\frac{TP}{TP+FN}.$$\n",
    "The precision can be interpreted as the ratio of true detections over all detections while the recall measures how many objects are detected of all given objects in the ground truth. \n",
    "\n",
    "Since the introduction of the PASCAL VOC (Everingham et al, 2010), the mean average precision (mAP) has been the de facto standard for measuring the performance of object detectors (Zou et al., 2019). The original average precision (AP) as defined by Everingham et al. (2010) is calculated as the mean precision over 11 different recall values:\n",
    "$$AP = \\frac{1}{11} \\sum_{r \\in \\{0,0.1,...,1\\}} p_{interp}(r)$$\n",
    "with $r$ being the recall value and $p_{interp}$ being the interpolated precision at $r$. The AP is then averaged over all available classes N to obtain the mAP:\n",
    "$$mAP = \\frac{1}{N} \\sum_{n \\in N} AP_n.$$\n",
    "Everingham et al. (2010) used a threshold of 0.5 for the IoU for their definition of the mAP which was later extended by the MS COCO Benchmark to the average of ten IoU thresholds (Lin et al, 2014):\n",
    "$$COCO\\;mAP = \\frac{1}{10} \\sum_{t \\in \\{0.5,0.55,...,0.95\\}} mAP_t.$$\n",
    "The COCO mAP has the advantage of rewarding detectors with better bounding box accuracy.\n",
    "\n",
    "As the problem on hand is a single class problem, we will not be needing the average over mutliple classes. However, since APIs for the COCO metrics are easily integrated into the TensorFlow object detection API, we will be using them.\n",
    "\n",
    "Sources:\n",
    "- Everingham, M., van Gool, L., Williams, C. K. I., Winn, J. and Zisserman, A. (2010),\n",
    "‘The pascal visual object classes (voc) challenge’, International Journal of Computer\n",
    "Vision 88(2), 303–338.\n",
    "- Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll ́ar, P.\n",
    "and Zitnick, C. L. (2014), Microsoft coco: Common objects in context, in D. Fleet,\n",
    "T. Pajdla, B. Schiele and T. Tuytelaars, eds, ‘Computer Vision – ECCV 2014’,\n",
    "Springer International Publishing, Cham, pp. 740–755.\n",
    "- Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll ́ar, P. and\n",
    "Zitnick, C. L. (n.d.a), ‘Evaluations of detections on the ms coco dataset’.\n",
    "URL: https://cocodataset.org/detection-eval\n",
    "- Zou, Z., Shi, Z., Guo, Y. and Ye, J. (2019), ‘Object detection in 20 years: A survey’.\n",
    "URL: http://arxiv.org/pdf/1905.05055v2\n",
    "## Model Training (TensorFlow OD API)\n",
    "The TensorFlow object detection API follows four basic steps for model training:\n",
    "### 1. Model Definition or Download\n",
    "First, a model has to either be defined from scratch (out of scope of this project), or downloaded. For this project, pretrained model checkpoints for popular OD-models are downloaded from the [TensorFlow object detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)\n",
    "### 2. Training Configuration\n",
    "Next, the training has to be configured in a training-pipeline config file. All aspects that controll the training are defined within this file. This includes general parameters such as batch size and learning rate, but also all the finer details such as data augmentation, model structure parameters, parallelization options, optimizer details as well as evaluation details. A example config file used for this project looks like this:\n",
    "```json\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 1\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 1024\n",
    "        width: 1024\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: \"ssd_resnet101_v1_fpn_keras\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.0004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.03\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.997\n",
    "          scale: true\n",
    "          epsilon: 0.001\n",
    "        }\n",
    "      }\n",
    "      override_base_feature_extractor_hyperparams: true\n",
    "      fpn {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "      }\n",
    "    }\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "        use_matmul_gather: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      weight_shared_convolutional_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.0004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            random_normal_initializer {\n",
    "              mean: 0.0\n",
    "              stddev: 0.01\n",
    "            }\n",
    "          }\n",
    "          activation: RELU_6\n",
    "          batch_norm {\n",
    "            decay: 0.997\n",
    "            scale: true\n",
    "            epsilon: 0.001\n",
    "          }\n",
    "        }\n",
    "        depth: 256\n",
    "        num_layers_before_predictor: 4\n",
    "        kernel_size: 3\n",
    "        class_prediction_bias_init: -4.6\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      multiscale_anchor_generator {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        anchor_scale: 4.0\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        scales_per_octave: 2\n",
    "      }\n",
    "    }\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-08\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "        use_static_shapes: false\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid_focal {\n",
    "          gamma: 2.0\n",
    "          alpha: 0.25\n",
    "        }\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    encode_background_as_zeros: true\n",
    "    normalize_loc_loss_by_codesize: true\n",
    "    inplace_batchnorm_update: true\n",
    "    freeze_batchnorm: false\n",
    "  }\n",
    "}\n",
    "train_config {\n",
    "  batch_size: 4\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  sync_replicas: true\n",
    "  optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.0025\n",
    "          total_steps: 15000\n",
    "          warmup_learning_rate: 0.00083333\n",
    "          warmup_steps: 1500\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "  fine_tune_checkpoint: \"/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/pretrained-models/ssd_resnet101_v1_fpn_1024x1024_coco17_tpu-8/checkpoint/ckpt-0\"\n",
    "  num_steps: 15000\n",
    "  startup_delay_steps: 0.0\n",
    "  replicas_to_aggregate: 4\n",
    "  max_number_of_boxes: 500\n",
    "  unpad_groundtruth_tensors: false\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "  use_bfloat16: true\n",
    "  fine_tune_checkpoint_version: V2\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/03_primary/label_map.pbtxt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/05_model_input/train.record\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/03_primary/label_map.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_epochs: 1\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/05_model_input/valid.record\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "The most important parameters for this project are the paths to the data, checkpoint and label map. \n",
    "### 3. Start Training and Evaluation Jobs\n",
    "The API is designed to run seperate processes for training and evaluation. Both can be started using a command similar to the one generated at the beginning of this notebook. The console log will show the progress of the training, but its more practical to use TensorBoard instead.\n",
    "### 4. Use TensorBoard to Monitor Training Progress\n",
    "The API is designed to log important loss parameters and evaluation metrics automatically. TensorBoard allows to conveniently view those metrics as they are generated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can also be viewed within a jupyter notebook by using an iframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1644137), started 0:01:33 ago. (Use '!kill 1644137' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-546dc9eaada727e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-546dc9eaada727e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results\n",
    "For a detailed view on model performace, check out the evaluation notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulties Encountered During Project Completion\n",
    "A multitude of problemes where encountered using the TensorFlow object detection API but also the Bizon server. This is also the reason why not nearly as many models and configurations could be tried in the available time. Some noteworthy problems that we encountered:\n",
    "- A bug in our data preparation pipeline took multiple days to find. The problem was that the bounding box coordinates weren't scaled correctly. \n",
    "- The TensorFlow object detection API relies heavily on the TensorFlow v1 compatibity mode of TF2. This caused a lot of problems during package installation and made it difficult to get a working conda environment.\n",
    "- TensorFlow doesn't make it easy to access its object detection capabilities. The documentation for the object detection API is of poor quality and lacks completeness. PyTorch would have been more appropriate for the task, as most public object detection architectures use it for training.\n",
    "- The Bizon server sometimes was at times fully occupied. This is especially frustrating if the occupant only utilizes the GPU, as the server is actually meant for GPU highly parallelized computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9763b797e91abcff88b30fb0158ca9b9621827488e019d1a51bbfc7ee01b7128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
