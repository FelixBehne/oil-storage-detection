{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb5117e-13f2-4e9d-848f-ae4118fa0305",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "### 1. Export model inference graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4691a96a-b870-4cb7-8eb8-5122d590ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter_main_v2_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/models/research/object_detection/exporter_main_v2.py'\n",
    "trained_checkpoint_dir_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/sliced_images_demo/models/my_efficientdet_d4_coco17_tpu-32/v2/'\n",
    "pipeline_config_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/sliced_images_demo/models/my_efficientdet_d4_coco17_tpu-32/v2/pipeline.config'\n",
    "output_directory_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/sliced_images_demo/inference_graph/my_efficientdet_d4_coco17_tpu-32/v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ffbd95-28ae-4495-9f61-898a5f44ca01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a14be-d759-45d9-98a6-049cb9c7b0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python {exporter_main_v2_path} \\\n",
    "    --trained_checkpoint_dir {trained_checkpoint_dir_path} \\\n",
    "    --output_directory {output_directory_path} \\\n",
    "    --pipeline_config_path {pipeline_config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf7e67-22ec-4bb5-b977-d917af6cfce3",
   "metadata": {},
   "source": [
    "## 2. Test trained model on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbfd8e8-36d3-4aa8-bd1f-a9c62c340bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import six\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import display\n",
    "\n",
    "from six import BytesIO\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdc8efb-7c2d-4044-b558-923f1b540647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: a file path (this can be local or on colossus)\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ab733f-e9b1-42d4-aa9a-e70490cee2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelmap_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/annotations/label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5892c9c-244b-40cf-97c9-7bb71968eebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebb8b29-8bab-4bfa-a5f7-b8add994e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff8a7f9-f373-469b-b10b-4e80fb172abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203a8d2d-a60b-45e8-9212-941dae0e8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 13:00:34.815096: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-25 13:00:39.853367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9583 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1c:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.855133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9646 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1d:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.856707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9646 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1e:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.858182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9646 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.859660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9646 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.861111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9646 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.862619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9646 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "2022-05-25 13:00:39.864073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9646 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:5e:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d060e05-67d7-41d1-b392-83c1de1a1653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fb71d3-2bab-4a1c-a66b-00defb516215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done! Took 38.47862386703491 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "model = tf.saved_model.load(f'{output_directory_path}/saved_model')\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660688e3-d7de-4cdc-8ee3-d4ef1833333c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a02f7-0b6c-45ca-91a3-5cadb930be08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c999f8c-1f18-45b1-bec6-735ffdaf0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca54681-a6a8-42b3-8a6c-876a66a6be61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e26006-4c30-426e-8923-6672641819be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50a2ee8-22d4-48cf-b21a-48d98736b790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12210ad7-83f8-4b54-bb4b-e93f8ff6ac1f_1_1_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "d3d2b706-9017-41f4-b57e-469038daa634_1_1_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "014de911-7810-4f7d-8967-3e5402209f4a_1_1_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "dacabde6-c8a7-475b-ab26-81b4e4dd5977_1_0_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "ca7b1077-0d37-4ada-9d56-0e66f864935e_0_1_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "b3324b5e-cca9-42f7-9806-ab1f86eee050_0_1_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "5bb8ab95-f141-43ca-a722-a1937d9d5a72_1_0_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "ed921ec7-b950-460c-a912-12247d867fea_1_0_0_0.jpg\n",
      "dict_keys(['detection_scores', 'raw_detection_scores', 'detection_anchor_indices', 'raw_detection_boxes', 'detection_classes', 'detection_multiclass_scores', 'detection_boxes', 'num_detections'])\n",
      "f82d64a6-3bfa-4612-bcbd-847a7d89c296_0_0_0_0.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m image_np \u001b[38;5;241m=\u001b[39m sliced\n\u001b[1;32m     31\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m run_inference_for_single_image(model, image_np)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mvis_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_boxes_and_labels_on_image_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_boxes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_masks_reframed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_thickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/bash/envs/DeepLearning/lib/python3.9/site-packages/object_detection/utils/visualization_utils.py:1250\u001b[0m, in \u001b[0;36mvisualize_boxes_and_labels_on_image_array\u001b[0;34m(image, boxes, classes, scores, category_index, instance_masks, instance_boundaries, keypoints, keypoint_scores, keypoint_edges, track_ids, use_normalized_coordinates, max_boxes_to_draw, min_score_thresh, agnostic_mode, line_thickness, mask_alpha, groundtruth_box_visualization_color, skip_boxes, skip_scores, skip_labels, skip_track_ids)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m   draw_mask_on_image_array(\n\u001b[1;32m   1245\u001b[0m       image,\n\u001b[1;32m   1246\u001b[0m       box_to_instance_boundaries_map[box],\n\u001b[1;32m   1247\u001b[0m       color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1248\u001b[0m       alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m   1249\u001b[0m   )\n\u001b[0;32m-> 1250\u001b[0m \u001b[43mdraw_bounding_box_on_image_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_boxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline_thickness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_str_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_to_display_str_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m   keypoint_scores_for_box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/bash/envs/DeepLearning/lib/python3.9/site-packages/object_detection/utils/visualization_utils.py:163\u001b[0m, in \u001b[0;36mdraw_bounding_box_on_image_array\u001b[0;34m(image, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates)\u001b[0m\n\u001b[1;32m    159\u001b[0m image_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39muint8(image))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n\u001b[1;32m    161\u001b[0m                            thickness, display_str_list,\n\u001b[1;32m    162\u001b[0m                            use_normalized_coordinates)\n\u001b[0;32m--> 163\u001b[0m np\u001b[38;5;241m.\u001b[39mcopyto(image, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pil\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/bash/envs/DeepLearning/lib/python3.9/site-packages/PIL/Image.py:677\u001b[0m, in \u001b[0;36mImage.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m--> 677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ArrayData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## Out-of-Sample Pictures\n",
    "#test_images_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/data_raw/extras/*jpg'\n",
    "\n",
    "\n",
    "## Eval-Pictures\n",
    "test_images_path = '/home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/sliced_images_demo/images/test/*jpg'\n",
    "\n",
    "\n",
    "# specify slice width=height\n",
    "slice_size = 1280\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    for image_path in glob.glob(test_images_path):\n",
    "        im = Image.open(image_path)\n",
    "        imr = np.array(im, dtype=np.uint8)\n",
    "        height = imr.shape[0]\n",
    "        width = imr.shape[1]\n",
    "\n",
    "        for i in range((height // slice_size)):\n",
    "            for j in range((width // slice_size)):\n",
    "                \n",
    "                sliced = imr[i*slice_size:(i+1)*slice_size, j*slice_size:(j+1)*slice_size]\n",
    "                #sliced_im = Image.fromarray(sliced)\n",
    "\n",
    "                filename = image_path.split('/')[-1]\n",
    "                slice_path = filename.replace('.jpg', f'_{i}_{j}.jpg')\n",
    "                print(slice_path)\n",
    "                \n",
    "                image_np = sliced\n",
    "                output_dict = run_inference_for_single_image(model, image_np)\n",
    "                vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np,\n",
    "                    output_dict['detection_boxes'],\n",
    "                    output_dict['detection_classes'],\n",
    "                    output_dict['detection_scores'],\n",
    "                    category_index,\n",
    "                    instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "                    use_normalized_coordinates=True,\n",
    "                    line_thickness=3)\n",
    "                print(output_dict.keys())\n",
    "                #display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9d112-69e6-4d1e-ad6a-9da752fb650f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957cbe28-f29a-4421-bb03-b26823c39a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove current session / unload current loaded model\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d608d6-fb76-4d2e-92c0-b9e1a40e9c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
