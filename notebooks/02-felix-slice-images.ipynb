{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables \n",
    "img_path = \"../data/01_raw/images/\"\n",
    "newpath = '../data/02_primary/images/quartered/true/'\n",
    "falsepath = '../data/02_primary/images/quartered/false/'\n",
    "slice_size = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list = glob.glob('../../data_raw/images/' + \"*.jpg\")\n",
    "df_res = pd.DataFrame(columns=[\"image_id\", \"geometry\", \"class\", \"bounds\",\"width\", \"height\"])\n",
    "\n",
    "# tile all images in a loop\n",
    "for imname in img_list:\n",
    "    im = Image.open(imname)\n",
    "    imr = np.array(im, dtype=np.uint8)\n",
    "    height = imr.shape[0]\n",
    "    width = imr.shape[1]\n",
    "\n",
    "    labels = df[df['image_id'] == imname.split('/')[-1]]\n",
    "    boxes = []\n",
    "    # convert bounding boxes to shapely polygons. We need to invert Y and find polygon vertices from center points\n",
    "    for index, row in labels.iterrows():\n",
    "        \n",
    "        x1, y1, x2, y2 = row[\"bounds\"][0], row[\"bounds\"][1], row[\"bounds\"][2], row[\"bounds\"][3]\n",
    "        \n",
    "       # x1 = row[1]['x1'] - row[1]['w']/2\n",
    "       # y1 = (height - row[1]['y1']) - row[1]['h']/2\n",
    "       # x2 = row[1]['x1'] + row[1]['w']/2\n",
    "       # y2 = (height - row[1]['y1']) + row[1]['h']/2\n",
    "        boxes.append((row['class'], Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)])))\n",
    "    \n",
    "    counter = 0\n",
    "    print('Image:', imname)\n",
    "    # create tiles and find intersection with bounding boxes for each tile\n",
    "    for i in range((height // slice_size)):\n",
    "        for j in range((width // slice_size)):\n",
    "            x1 = (j*slice_size)\n",
    "            y1 = (i*slice_size)\n",
    "            x2 = ((j+1)*slice_size) - 1\n",
    "            y2 = ((i+1)*slice_size) - 1\n",
    "                        \n",
    "            pol = Polygon([(x1, y1), (x2, y1), (x2, y2), (x1, y2)])\n",
    "            \n",
    "            imsaved = False\n",
    "            slice_labels = []\n",
    "            slice_anno = []\n",
    "\n",
    "            for box in boxes:\n",
    "                if pol.intersects(box[1]):\n",
    "                    inter = pol.intersection(box[1])   \n",
    "                    if not imsaved:\n",
    "                        sliced = imr[i*slice_size:(i+1)*slice_size, j*slice_size:(j+1)*slice_size]\n",
    "                        sliced_im = Image.fromarray(sliced)\n",
    "                        filename = imname.split('/')[-1]\n",
    "                        slice_path = newpath + filename.replace('.jpg', f'_{i}_{j}.jpg')\n",
    "                        \n",
    "                        slice_labels_path = newpath + filename.replace('.jpg', f'_{i}_{j}.txt')\n",
    "                        \n",
    "                        print(slice_path)\n",
    "                        sliced_im.save(slice_path)\n",
    "                        imsaved = True                    \n",
    "                    \n",
    "                    # get the smallest polygon (with sides parallel to the coordinate axes) that contains the intersection\n",
    "                    new_box = inter.envelope \n",
    "                    # get central point for the new bounding box \n",
    "                    centre = new_box.centroid\n",
    "                    # get coordinates of polygon vertices\n",
    "                    x, y = new_box.exterior.coords.xy\n",
    "                    \n",
    "                    \n",
    "                    # if the bbox changed due to the image split, this means the airplane is truncated so the correct class will be set\n",
    "                    objClass = str(box[0])\n",
    "                    if(box[1].exterior.coords.xy != new_box.exterior.coords.xy):\n",
    "                        objClass = \"Truncated_airplane\"\n",
    "                        \n",
    "                    # get bounding box width and height normalized to slice size\n",
    "                    #new_width = (max(x) - min(x)) / slice_size\n",
    "                    #new_height = (max(y) - min(y)) / slice_size\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # we have to normalize central x and invert y for yolo format\n",
    "                    #new_x = (centre.coords.xy[0][0] - x1) / slice_size\n",
    "                    #new_y = (y1 - centre.coords.xy[1][0]) / slice_size\n",
    "                    \n",
    "                    counter += 1\n",
    "\n",
    "                    #slice_labels.append([box[0], new_x, new_y, new_width, new_height])\n",
    "                    \n",
    "                    \n",
    "                    #if(min(x) > box[1][1][0]  )\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    min_x, max_x, min_y, max_y = math.ceil(min(x) - (j*slice_size)), math.ceil(max(x)- (j*slice_size)), math.ceil(min(y)- (i*slice_size)), math.ceil(max(y)- (i*slice_size))\n",
    "                    slice_width, slice_height = max_x - min_x, max_y - min_y \n",
    "                    geometry = [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]\n",
    "                    bounds= (min_x, min_y, max_x, max_y)\n",
    "                    \n",
    "                    slice_anno.append([imname.split('/')[-1].replace('.jpg', f'_{i}_{j}.jpg'), geometry, objClass, bounds, slice_width, slice_height])\n",
    "            \n",
    "                    \n",
    "            \n",
    "            # save txt with labels for the current tile\n",
    "            if len(slice_anno) > 0:\n",
    "                #slice_df = pd.DataFrame(slice_labels, columns=['class', 'x1', 'y1', 'w', 'h'])\n",
    "                #slice_df.to_csv(slice_labels_path, sep=' ', index=False, header=False, float_format='%.6f')\n",
    "                \n",
    "                df_anno = pd.DataFrame(slice_anno, columns=[\"image_id\", \"geometry\", \"class\", \"bounds\",\"width\", \"height\"])\n",
    "                df_res = df_res.append(df_anno)\n",
    "            \n",
    "            # if there are no bounding boxes intersect current tile, save this tile to a separate folder \n",
    "            if not imsaved:\n",
    "                sliced = imr[i*slice_size:(i+1)*slice_size, j*slice_size:(j+1)*slice_size]\n",
    "                sliced_im = Image.fromarray(sliced)\n",
    "                filename = imname.split('/')[-1]\n",
    "                slice_path = falsepath + filename.replace('.jpg', f'_{i}_{j}.jpg')                \n",
    "\n",
    "                sliced_im.save(slice_path)\n",
    "                print('Slice without boxes saved')\n",
    "                imsaved = True\n",
    "df = df_res.reset_index(drop=True) \n",
    "df.to_csv('../sliced_images/quartered/annotaion/annotation.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['image_id'] == '074737f4-7f59-4729-be5d-67f6f1d34668_1_0.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import PIL.ImageDraw\n",
    "\n",
    "imList = ['../sliced_images/quartered/true/074737f4-7f59-4729-be5d-67f6f1d34668_0_0.jpg', \n",
    "          '../sliced_images/quartered/true/074737f4-7f59-4729-be5d-67f6f1d34668_1_0.jpg', '../sliced_images/quartered/true/074737f4-7f59-4729-be5d-67f6f1d34668_1_1.jpg']\n",
    "\n",
    "for pickone in imList:\n",
    "    \n",
    "    img = PIL.Image.open(pickone)\n",
    "    draw = PIL.ImageDraw.Draw(img)\n",
    "\n",
    "    for k, row in df[df['image_id'] == os.path.basename(pickone)].iterrows():\n",
    "        draw.polygon(row['geometry'], outline=(255,0,0))\n",
    "        draw.text(row['geometry'][0], row['class'], fill=(255,0,0))\n",
    "\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split\n",
    "---\n",
    "run and save bounding boxes on training images and save training and test images seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createBoundingBoxesOnImages = 1\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = df[[\"image_id\", \"geometry\", \"bounds\", \"width\", \"height\"]]\n",
    "y = df[['class']]\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.2, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=df.image_id))\n",
    "\n",
    "X_train = X.loc[train_ix]\n",
    "y_train = y.loc[train_ix]\n",
    "\n",
    "X_test = X.loc[test_ix]\n",
    "y_test = y.loc[test_ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(createBoundingBoxesOnImages == 1):\n",
    "    for id in X_train[\"image_id\"].unique():\n",
    "        pickone = '../sliced_images/quartered/true/' + id\n",
    "        img = PIL.Image.open(pickone)\n",
    "        #draw = PIL.ImageDraw.Draw(img)\n",
    "        #for k, row in df[df['image_id'] == id].iterrows():\n",
    "        #    draw.polygon(row['geometry'], outline=(255,0,0))\n",
    "        #    draw.text(row['geometry'][0], row['class'], fill=(255,0,0))\n",
    "        \n",
    "        img.save('../images/train/' + id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(createBoundingBoxesOnImages == 1):\n",
    "    for id in X_test[\"image_id\"].unique():\n",
    "        pickone = '../sliced_images/quartered/true/' + id\n",
    "        img = PIL.Image.open(pickone)\n",
    "        \n",
    "        img.save('../images/test/' + id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = list('../images/train/*.jpg')\n",
    "print(\"Number of Train-images: {}\".format(len(img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create XML Files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xml.etree.cElementTree as ET\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def create_labimg_xml(image_path, df):\n",
    "\n",
    "    temp_df = df\n",
    "    image_path = Path(image_path)\n",
    "    img = np.array(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "    annotation = ET.Element('annotation')\n",
    "    ET.SubElement(annotation, 'folder').text = str(image_path.parent.name)\n",
    "    ET.SubElement(annotation, 'filename').text = str(image_path.name)\n",
    "    ET.SubElement(annotation, 'path').text = str(image_path)\n",
    "\n",
    "    source = ET.SubElement(annotation, 'source')\n",
    "    ET.SubElement(source, 'database').text = 'Unknown'\n",
    "\n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "    ET.SubElement(size, 'width').text = str (img.shape[1])\n",
    "    ET.SubElement(size, 'height').text = str(img.shape[0])\n",
    "    ET.SubElement(size, 'depth').text = str(img.shape[2])\n",
    "\n",
    "    ET.SubElement(annotation, 'segmented').text = '0'\n",
    "\n",
    "    for index, row in temp_df.iterrows():\n",
    "        xmin, ymin, xmax, ymax = row[\"bounds\"][0], row[\"bounds\"][1], row[\"bounds\"][2], row[\"bounds\"][3]\n",
    "\n",
    "        object = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(object, 'pose').text = 'Unspecified'\n",
    "        if(row[\"class\"] == \"Truncated_airplane\"):\n",
    "            ET.SubElement(object, 'name').text = \"Airplane\"\n",
    "            ET.SubElement(object, 'truncated').text = '1'\n",
    "        elif(row[\"class\"] == \"Airplane\"):\n",
    "            ET.SubElement(object, 'name').text = \"Airplane\"\n",
    "            ET.SubElement(object, 'truncated').text = '0'\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "        ET.SubElement(object, 'difficult').text = '0'\n",
    "\n",
    "        bndbox = ET.SubElement(object, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(xmin)\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(ymin)\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(xmax)\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(ymax)\n",
    "\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    xml_file_name = image_path.parent / (image_path.name.split('.')[0]+'.xml')\n",
    "    tree.write(xml_file_name)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# a quadrilateral bounding box(8 points) coordinate example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in X_train[\"image_id\"].unique():\n",
    "        image_id = '../images/train/' + id\n",
    "        df_temp = df[df['image_id'] == id]\n",
    "        create_labimg_xml(image_id, df_temp)\n",
    "        \n",
    "for id in X_test[\"image_id\"].unique():\n",
    "        image_id = '../images/test/' + id\n",
    "        df_temp = df[df['image_id'] == id]\n",
    "        create_labimg_xml(image_id, df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "nohup python model_main_tf2.py --model_dir=models/my_efficientdet_d7_coco17_tpu-32/v1 --pipeline_config_path=models/my_efficientdet_d7_coco17_tpu-32/v1/pipeline.config --checkpoint_dir=models/my_efficientdet_d7_coco17_tpu-32/v1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert XML to Tensorflow Record\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create train data:\n",
    "python generate_tfrecord.py -x /Users/lig/git/schule/airbus_aircraft_etection/tensorflow_env/TensorFlow/workspace/training_demo/images/train -l /Users/lig/git/schule/airbus_aircraft_etection/tensorflow_env/TensorFlow/workspace/training_demo/annotations/label_map.pbtxt -o /Users/lig/git/schule/airbus_aircraft_etection/tensorflow_env/TensorFlow/workspace/training_demo/annotations/train.record\n",
    "\n",
    "### Create test data:\n",
    "python generate_tfrecord.py -x /Users/lig/git/schule/airbus_aircraft_etection/tensorflow_env/TensorFlow/workspace/training_demo/images/test -l /Users/lig/git/schule/airbus_aircraft_etection/tensorflow_env/TensorFlow/workspace/training_demo/annotations/label_map.pbtxt -o /Users/lig/git/schule/airbus_aircraft_etection/tensorflow_env/TensorFlow/workspace/training_demo/annotations/test.record\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "python generate_tfrecord.py -x /home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/images/test -l /home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/annotations/label_map.pbtxt -o /home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/annotations/test.record\n",
    "\n",
    "python generate_tfrecord.py -x /home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/images/train -l /home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/annotations/label_map.pbtxt -o /home/zengerle@ab.ba.ba-ravensburg.de/airplane_detection/TensorFlow/workspace/training_demo/annotations/train.record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "---\n",
    "### Navigate to airplane_detection/TensorFlow/workspace/training_demo:\n",
    "\n",
    "#### Default Training-Script:\n",
    "python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn/v1 --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/v1/pipeline.config &\n",
    "\n",
    "#### Run Training in Background with Terminal Outputstream\n",
    "nohup python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn/v1 --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/v1/pipeline.config & \\\n",
    "(Access Stream with: tail -f nohup.out)\n",
    "\n",
    "#### Run Training in Background and save stream to log-file:\n",
    "nohup python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn/v1 --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/v1/pipeline.config --alsologtostderr > /log/my_ssd_resnet50_v1_fpn/v1/train.log 2>&1 & \\\n",
    "(Access Log-File with: tail -f /log/model_main_tf2.log)\n",
    "\n",
    "\n",
    "#### Note:\n",
    "wenn die Batch-Size drastisch verringert wird (z.B. 128 -> 2 = Faktor x64) muss auch die Learning-Rate dementsprechend angepasst werden (ansonsten divergiert Loss-Function -> Division durch 0 -> Loss von NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the trained Modell on latest Checkpoints\n",
    "---\n",
    "### Navigate to airplane_detection/TensorFlow/workspace/training_demo:\n",
    "\n",
    "\n",
    "##### Evaluate with the latest Model-Checkpoint:\n",
    "nohup python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn/v1 --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/v1/pipeline.config --checkpoint_dir=models/my_ssd_resnet50_v1_fpn/v1 &\n",
    "\n",
    "#### Note:\n",
    "--checkpoint_dir will only execute the evaluation -> no training \\\n",
    "Run parallel / at the same time to the Training e.g. in new terminal to evaluate all the checkpoints over the whole trainings process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Additional Features:\n",
    "---\n",
    "### Navigate to airplane_detection/TensorFlow/workspace/training_demo:\n",
    "\n",
    "\n",
    "#### Tensorboard - Visualize Training / Evaluation:\n",
    "Start Tensorboard: \\\n",
    "tensorboard --logdir=models/my_ssd_resnet50_v1_fpn/v1/ &\n",
    "\n",
    "#### Get nohup-stream:\n",
    "tail -f nohup.out\n",
    "\n",
    "#### Get GPU-Info:\n",
    "nvidia-smi \\\n",
    "watch -n 1 nvidia-smi // live update every second\n",
    "\n",
    "#### Set available GPUs (GPUs which should be used for the program):\n",
    "export CUDA_VISIBLE_DEVICES=0 // set device by single indizes \\\n",
    "export CUDA_VISIBLE_DEVICES=3,4,5 // set list of devices \\\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 // all devices \n",
    "\n",
    "#### Get Task-Manager\n",
    "top\n",
    "\n",
    "#### List servers\n",
    "jobs -l\n",
    "\n",
    "#### Kill process:\n",
    "kill \"pid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "\n",
    "#### Hyperparametertuning\n",
    "Parameter anschauen, in voller Auflösung trainieren 1280x1280\n",
    "\n",
    "#### Eval-Bilder Predicten und anschauen \n",
    "\n",
    "#### Modell-Ergebnisse an UseCase anpassen\n",
    "Ergebniss bewerten, auf UseCase beziehen, was uns das nun bringt, die Flugzeuge zu erkennen (Anzahl an predicteten Flugzeugen, ...)\n",
    "\n",
    "#### Ergebnisse visualisieren\n",
    "Count der Flugzeuge in e.g. Datafrage zu image-id \\\n",
    "Histogram - verteilung der Anzahl an Flugzeugen \\\n",
    "Durchschnitt an Flugzeugen pro Flughafen etc.\n",
    "\n",
    "\n",
    "#### truncated / abgeschnittene Flugzeuge\n",
    "auf slices predicten und danach vier slices wieder zu einem Bild zusammenfügen\n",
    "\n",
    "#### Test - Train Split ändern (z.B. 90 - 10 für mehr Trainingsbilder)\n",
    "\n",
    "#### Ausblick wie man Ergebnis verbessern kann\n",
    "mehr Trainingsbilder (Data Augmentation) -> Spiegeln, verzerren, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "navigate to (training_demo)\n",
    "\n",
    "nohup python model_main_tf2.py --model_dir=models/conf_ssd_resnet50_v1_fpn --pipeline_config_path=models/conf_ssd_resnet50_v1_fpn/pipeline.config &\n",
    "\n",
    "\n",
    "(sollte sehr genau sein dauert aber lange)\n",
    "nohup python model_main_tf2.py --model_dir=models/my_centernet_hg104_1024x1024_coco17_tpu-32 --pipeline_config_path=models/my_centernet_hg104_1024x1024_coco17_tpu-32/pipeline.config &\n",
    "\n",
    "\n",
    "(sollte sehr schnell sein ist aber nicht so genau)\n",
    "nohup python model_main_tf2.py --model_dir=models/my_centernet_mobilenetv2_fpn_od --pipeline_config_path=models/my_centernet_mobilenetv2_fpn_od/pipeline.config &\n",
    "\n",
    "\n",
    "(sollte sehr genau sein dauert aber lange)\n",
    "nohup python model_main_tf2.py --model_dir=models/my_efficientdet_d7_coco17_tpu-32 --pipeline_config_path=models/my_efficientdet_d7_coco17_tpu-32/pipeline.config &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
  },
  "kernelspec": {
   "display_name": "osd",
   "language": "python",
   "name": "osd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.734687,
   "end_time": "2021-03-12T08:59:01.880112",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-12T08:58:42.145425",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
