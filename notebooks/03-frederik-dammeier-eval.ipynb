{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb5117e-13f2-4e9d-848f-ae4118fa0305",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "### 1. Export Model\n",
    "Before the Model can be evaluated properly, it has to be exported from the most recent checkpoint. This is done by the `exporter_main_v2.py` script of the TensorFlow object detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691a96a-b870-4cb7-8eb8-5122d590ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter_main_v2_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/scripts/exporter_main_v2.py'\n",
    "trained_checkpoint_dir_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/checkpoint/'\n",
    "pipeline_config_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/pipeline.config'\n",
    "output_directory_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ffbd95-28ae-4495-9f61-898a5f44ca01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit this notebook to one GPU in order to preseve resources for other users.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a14be-d759-45d9-98a6-049cb9c7b0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python {exporter_main_v2_path} \\\n",
    "    --trained_checkpoint_dir {trained_checkpoint_dir_path} \\\n",
    "    --output_directory {output_directory_path} \\\n",
    "    --pipeline_config_path {pipeline_config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ffcba",
   "metadata": {},
   "source": [
    "## 2. Recap of the Model Training\n",
    "\n",
    "### Model Information\n",
    "We trained a SSD (Single Stage Detector) model using a Resnet101 Backbone and a 1024x1024 input layer on quatered input images (resulting in a size of 1280x1280) for the input images. The images thus had to be slightly scales during model training. The model is pretrained on COCO-2017 using the checkpoints provided in the TensorFlow model Zoo. We use a batch size of four on four GPUs, a learning rate of 0.0025 with momentum optimizer and a toal of 15000 steps (=trained Batches).\n",
    "\n",
    "### Training Progress\n",
    "Before we actually test the final model on single images and the test dataset, let's first discuss what information can be obtained from the TensorBoard tracking information during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TensorBoard in notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/06_models/models/ssd_resnet101_1024/v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375b558",
   "metadata": {},
   "source": [
    "### Training Progress Analysis\n",
    "A few interesting things can be told from the training graphs. First, lets explain some basics. The orange graphs show the metrics calculated from the performance on the train dataset. The blue graphs are on the eval dataset. It is clear that all the loss values are much lower on the training set than on the validation set, which suggests bad generalization capabilitie of the model, but we will have to look into that later. Interesting are the graphs depicting the different components of the COCO detection metrics (mAP) which was introduced in the training notebook. It is clear that the performance on the validation set reaches a maximum after about 15000 training steps after which it drops again, suggesting overfitting. It can also be observed that the filter for large objects that is provided by the COCO detection API is constantly at -1 mAP which means that the dataset does not contain any large objects. This is in line with the findings in the EDA notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf7e67-22ec-4bb5-b977-d917af6cfce3",
   "metadata": {},
   "source": [
    "## 2. Test trained model on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfd8e8-36d3-4aa8-bd1f-a9c62c340bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import six\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from six import BytesIO\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac37a8",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "First, the model, that we previously generated has to be imported into tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc284405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "model = tf.saved_model.load(f'{output_directory_path}/saved_model')\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc8efb-7c2d-4044-b558-923f1b540647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: a file path (this can be local or on colossus)\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6113dc7",
   "metadata": {},
   "source": [
    "### Run Inferece on Entire Test-set\n",
    "We now infere results on the test set and calculate the coco-metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e12f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_aggregate_detections(model, image_path):\n",
    "    model_fn = model.signatures['serving_default']\n",
    "    results = []\n",
    "\n",
    "    for image_name in glob.glob(image_path):\n",
    "        # img_pillow = Image.open(image_name)\n",
    "        # img_numpy = np.array(img_pillow.getdata()).reshape(img_pillow.size[0], img_pillow.size[1], 3).astype(np.uint8)\n",
    "        img_numpy = load_image_into_numpy_array(image_name)\n",
    "\n",
    "        img_tensor = tf.convert_to_tensor(img_numpy)\n",
    "\n",
    "        img_tensor = img_tensor[tf.newaxis,...]\n",
    "\n",
    "        output_dict = model_fn(img_tensor)\n",
    "\n",
    "        # All outputs are batches tensors.\n",
    "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "        # We're only interested in the first num_detections.\n",
    "        num_detections = int(output_dict.pop('num_detections'))\n",
    "        output_dict = {key:value[0, :num_detections].numpy() \n",
    "                    for key,value in output_dict.items()}\n",
    "\n",
    "        results.append(output_dict)\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2919c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/04_feature/images/test/*jpg'\n",
    "\n",
    "results = run_inference_aggregate_detections(model, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.core import standard_fields as fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import ground truth\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "def import_ground_truth(annotations_file_path):\n",
    "\n",
    "    annotations = pd.read_csv(annotations_file_path)\n",
    "\n",
    "    def expand_bbox_coordinates(row: pd.Series):\n",
    "        bbox_as_tuple = literal_eval(row[\"bounds\"])\n",
    "        row[\"bbox_x1\"] = bbox_as_tuple[0]\n",
    "        row[\"bbox_y1\"] = bbox_as_tuple[1]\n",
    "        row[\"bbox_x2\"] = bbox_as_tuple[2]\n",
    "        row[\"bbox_y2\"] = bbox_as_tuple[3]\n",
    "\n",
    "        return row\n",
    "\n",
    "    annotations = annotations.apply(expand_bbox_coordinates, 1)\n",
    "\n",
    "    image_names = annotations.image_id.unique()\n",
    "    annotations_list = []\n",
    "\n",
    "    for image_name in image_names:\n",
    "        bboxes = []\n",
    "\n",
    "        annotations_filtered = annotations[\n",
    "            annotations[\"image_id\"] == image_name\n",
    "        ]\n",
    "        xmins = annotations_filtered[\"bbox_x1\"].values\n",
    "        xmaxs = annotations_filtered[\"bbox_x2\"].values\n",
    "        ymins = annotations_filtered[\"bbox_y1\"].values\n",
    "        ymaxs = annotations_filtered[\"bbox_y2\"].values\n",
    "\n",
    "        for i in range(len(xmins)):\n",
    "            bboxes.append([ymins[i], xmins[i], ymaxs[i], xmaxs[i]])\n",
    "\n",
    "        bboxes = np.array(bboxes, dtype='float32')\n",
    "        \n",
    "        annotations_list.append(\n",
    "            {fields.InputDataFields.groundtruth_boxes: bboxes, fields.InputDataFields.groundtruth_classes: np.array([1 for _ in range(len(bboxes))], dtype='int64')}\n",
    "        )\n",
    "\n",
    "    return image_names, annotations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83455a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names, annotations_list = import_ground_truth(\"../data/04_feature/annotations/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results, image_size):\n",
    "    results_new = []\n",
    "\n",
    "    for result in results:\n",
    "        results_new.append(\n",
    "            {\n",
    "                fields.DetectionResultFields.detection_boxes: result[\"detection_boxes\"]*image_size,\n",
    "                fields.DetectionResultFields.detection_scores: result[\"detection_scores\"],\n",
    "                fields.DetectionResultFields.detection_classes: result[\"detection_classes\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ce76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_results = process_results(results, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb897302",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = [\n",
    "    filename.split(\"/\")[-1].split(\".\")[0]\n",
    "    for filename in glob.glob(img_path)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.metrics.coco_evaluation import CocoDetectionEvaluator\n",
    "\n",
    "cde = CocoDetectionEvaluator([{\"id\": 1, \"name\": \"OST\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d333f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(image_list)):\n",
    "    cde.add_single_ground_truth_image_info(\n",
    "        image_names[i], annotations_list[i]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(image_list)):\n",
    "    cde.add_single_detected_image_info(\n",
    "            image_list[i], processed_results[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = cde.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31faa745",
   "metadata": {},
   "source": [
    "### Single Image Intferece\n",
    "Alternatively, we can display the boxes on top of the images. Code copied from Zengerle & GÃ¶hl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab733f-e9b1-42d4-aa9a-e70490cee2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelmap_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/04_feature/annotations/label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c999f8c-1f18-45b1-bec6-735ffdaf0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a2ee8-22d4-48cf-b21a-48d98736b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = '/home/dammeier@ab.ba.ba-ravensburg.de/dev/oil-storage-detection/data/04_feature/images/test/*jpg'\n",
    "\n",
    "image_paths = glob.glob(test_images_path)\n",
    "\n",
    "image_path = image_paths[3]\n",
    "\n",
    "#for image_path in glob.glob(test_images_path):\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "output_dict = run_inference_for_single_image(model, image_np)\n",
    "vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np,\n",
    "    output_dict['detection_boxes'],\n",
    "    output_dict['detection_classes'],\n",
    "    output_dict['detection_scores'],\n",
    "    category_index,\n",
    "    instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "    use_normalized_coordinates=True,\n",
    "    line_thickness=8)\n",
    "display(Image.fromarray(image_np))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9763b797e91abcff88b30fb0158ca9b9621827488e019d1a51bbfc7ee01b7128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
